from max import engine
from transformers import AutoTokenizer, AutoModelForCausalLM
from String import String

def generate(model_path, input_ids):
    session = engine.InferenceSession()
    maxmodel = session.load(f"{model_path}")

    print("\nGenerating text...")

    # Values used in generation loop
    inputs = {}
    N_BATCH = 1
    N_LAYERS = 32
    N_HEADS = 8
    KV_LEN = 128

    # Initialize the additional layer to 0 for the first iteration:
    for i in range(N_LAYERS):
        inputs[f"past_key_values.{i}.key"] = torch.zeros(
            [N_BATCH, N_HEADS, 0, KV_LEN], dtype=torch.float
        ).numpy()
        inputs[f"past_key_values.{i}.value"] = torch.zeros(
            [N_BATCH, N_HEADS, 0, KV_LEN], dtype=torch.float
        ).numpy()

    current_seq = input_ids

    logits_processor = LogitsProcessorList()
    for idx in range(N_TOKENS):
        # Prepare inputs dictionary
        inputs["input_ids"] = current_seq.numpy()
        inputs["position_ids"] = (
            torch.arange(inputs["input_ids"].shape[1], dtype=torch.long)
            .unsqueeze(0)
            .numpy()
        )
        inputs["attention_mask"] = torch.ones(
            [
                1,
                inputs[f"past_key_values.0.key"].shape[2]
                + inputs["input_ids"].shape[1],
            ],
            dtype=torch.int64,
        ).numpy()

        # Run the model with MAX engine
        max_outputs = maxmodel.execute(**inputs)
        outputs = torch.from_numpy(max_outputs["logits"])

        # Get the newly generated next token
        next_token_logits = outputs[:, -1, :]
        next_tokens_scores = logits_processor(current_seq, next_token_logits)
        next_tokens = torch.argmax(next_tokens_scores, dim=-1)

        # Append the new token to our sequence
        current_seq = torch.cat([current_seq, next_tokens[:, None]], dim=-1)

        # Update the KV cache for the next iteration
        for i in range(N_LAYERS):
            inputs[f"past_key_values.{i}.key"] = max_outputs[f"present.{i}.key"]
            inputs[f"past_key_values.{i}.value"] = max_outputs[
                f"present.{i}.value"
            ]

    return current_seq.numpy()


def main():
    var model_name = String("casperhansen/mixtral-instruct-awq")
    var model_dir = String("./mixtral-instruct-awq")

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)

    tokenizer = AutoTokenizer.from_pretrained(model_name)
    tokenizer.pad_token = tokenizer.eos_token

    input_ids = tokenizer(
            "what is the 5th day of the week?", return_tensors="pt", max_length=128, truncation=True
        ).input_ids
    print("Input processed.\n")

    outputs = generate(model_dir, input_ids)
    print("Text generated.\n")

    response = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0][
        len(args.text) :
    ]

    print(f"Prompt: {args.text}")
    print(f"Response: {response}")


if __name__ == "__main__":
    main()